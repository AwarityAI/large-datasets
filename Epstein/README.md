# Epstein Emails
The goal of this project is to reveal the truth. Our technology is able to reason over very large datasets. The largest dataset we've reasoned over is 450m tokens (337m words) and it took 2 days to resolve a query. The Epstein emails are relatively small at 14m tokens. It takes about 10 minutes to run a query over this dataset. We're hoping that the community will help us decide what to search for and we will share everything we find.

The first thing we do with any dataset is we run a classification task over the files. We always want to reason over a subset of files so what we want to do is put all of the files into buckets. We have a Command Line Interface (CLI) tool that we run over the files in a dataset (corpus). Our CLI is called Awareness and it has a broad set of capabilities. The "classify" function is super powerful and has no limit for the number of files it can classify. This corpus had almost 2,900 files and it took 15 hours to label all of those files. You could have a million files and it would just be a matter of spending the time to classify each document. The classify task is about assigning labels to every file. Out of 2900 files the model thought that 2352 files were interesting. It then assigned 7973 labels to those files. 

We used Chat GPT to identify 12 categories that seemed interesting and we then ran our classification task over all of the files, mapping each file to one or more of those labels. The output was a spreadsheet which you can think of as a map for what files to read by topic. We use this spreadsheet to filter the corpus. Instead of having to reason over 14m tokens we're typically able to filter the corpus down to 5m tokens based on the category we care about. Thats still a lot but with the Elastic Context Window (ECW) we can typically resolve a query in about 10 minutes. Even at 5m tokens we're typically making 2,000 model calls and we end up processing around 7m tokens. The ECW has a bit of overhead...

The thing to understand about reasoning is that its mostly a compression task. We're taking that 5m tokens and compressing it as tightly as we can to be relevant to the question we're asking. Once we've compressed the corpus as tight as we can we can then present it to the model and decompress the text into an answer. It's actually su[er simple. The magic is in how we compress the text. We can typically compress the corpus by 98%.

]