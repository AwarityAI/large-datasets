# Epstein Emails
Our technology is able to reason over very large datasets. The largest dataset we've reasoned over is 450m tokens (337m words) and it took 2 days to resolve a query. The Epstein emails are relatively small at 14m tokens (although still larger than what any major LLM like ChatGPT can do today). It takes about 10 minutes to run a query over this dataset. We're hoping that the community will help us decide what to search for and we will share everything we find.

The first thing we do with any dataset is we run a classification task over the files. We always want to reason over a subset of files so what we want to do is put all of the files into buckets. We have a Command Line Interface (CLI) tool that we run over the files in a dataset. The "classify" function is powerful and has no limit for the number of files it can classify. This dataset had almost 2,900 files and it took 15 hours to label all of those files. You could have a million files and it would just be a matter of spending the time to classify each document. Out of 2900 files the model thought that 2352 files were interesting. It then assigned 7973 labels to those files. 

We used a standard LLM (in this case OpenAI's GPT-5) to identify 12 categories that seemed interesting and we then ran our classification task over all of the files, mapping each file to one or more of those labels. The output was a spreadsheet which you can think of as a map for what files to read by topic. We use this spreadsheet to filter the dataset. For a dataset this size we can typically filter it down to around 5m tokens. Thats still a lot but with the Elastic Context Window (ECW) we can typically resolve a 5m token query in about 10 minutes.
